---
title: "class06_webscraping"
author: "Cam Townswick"
date: "2026-02-16"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE
    code_folding: show
    code_download: TRUE
    theme: simplex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Example for site 1
robotstxt::paths_allowed(
  paths  = "/ooh/", domain = "www.bls.gov", bot    = "*"
)
```

```{r}
# Example for site 2
robotstxt::paths_allowed(
  paths  = "/jobs/", domain = "www.indeed.com", bot    = "*"
)
# warning event: on_file_type_mismatch comes from the server returning an HTML page or JSON, sometimes b/c url redirected to somewhere else
# other warnings you can look up again using AI ! moving on...
```

```{r}
# Example for site 3
robotstxt::paths_allowed(
  paths  = "/jobs/", domain = "www.linkedin.com", bot    = "*"
)
```
# Scraping the BLS Webpage

In this example, we will be scraping the [Top Growing Jobs BLS Web Page](https://www.bls.gov/emp/tables/fastest-growing-occupations.htm).

```{r}
# step 0: am I allowed to scrape this webpage
# this returned TRUE, which means I am allowed to scrape the webpage
robotstxt::paths_allowed(
  paths = "/emp/",
  domain = "https://www.bls.gov"
)

# step 1: get the back end HTML page into R
# for this example, the simple rvest::read_html() failed with a "cannot open the connection" error, so we will start by declaring ourselves
# reminder, us "?httr2::request" in console for help guide to the right

# bls_fail = rvest::read_html(
#   "https://www.bls.gov/emp/tables/fastest-growing-occupations.htm"
# )

url <- "https://www.bls.gov/emp/tables/fastest-growing-occupations.htm"

resp =
  httr2::request(url) |>
  httr2::req_user_agent("An ISA 401 Student Scraper from Miami University") |> 
  # httr2::req_retry(max_tries = 3) |>
  # httr2::req_timeout(60) |>
  httr2::req_perform()

# bls_webpage = response |>
#   httr2::req_body_raw() |> 
#   rvest::read_html()

# Error in `httr2::req_body_raw()`:
# ! `req` must be an HTTP request object, not a <httr2_response> object.
# Run `rlang::last_trace()` to see where the error occurred.

# try it again with an edit!

# if the previous step works correctly
# page will be a list of 2 that contains:
# (a) head [metadata]; and (b) body[ things that we see]

page = resp |>
  httr2::resp_body_raw() |>
  rvest::read_html()

names(page) # this returns the names within the page -- node, doc
print(page) # this returns the head and body of the webpage

# step 2: from the page, extract the part of interest
table_1 = page |> rvest::html_elements("#BLStable_2025_7_18_12_5")
# command + shift shows the html language, but table_1_clean shows up as readable to humans

# step 3: we make this human readable
table_1_clean = table_1 |> rvest::html_table()
table_1_clean = table_1_clean[[1]]
head(table_1_clean)
```

# Alternate aproaches for pulling that table from "page"

```{r}
t1_b = # the page is step 1
  page |>
  # step 2
  rvest::html_elements("table") |>
  # step 3
  rvest::html_table() |>
  magrittr::extract2(1) # the extract2 fn is equivalent to the [[]]
t1_b

# this is an example (rvest::html_elements(".regular") |>) of using the CSS selector to get all elements from the table in inspect, table is named "table#BLStable_2025_7_18_12_5.regular" so using .regular, it just pulls the html_elements within the table
# t1_b = # the page is step 1
#   page |>
#   # step 2
#   rvest::html_elements(".regular") |>
#   # step 3
#   rvest::html_table() |>
#   magrittr::extract2(1) # the extract2 fn is equivalent to the [[]]
```








